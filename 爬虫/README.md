## 知乎百万级用户数据爬虫

​	最近一直在忙考研的事情，所以一直没怎么更新，但这次来了个大的~~，之前有朋友在issue中提出要我研究一下微信公众号的爬虫，这在无形中提醒了我之前想做的一个事情——知乎用户爬虫【之前在做问答爬虫的时候就在考虑这个事情了】。可由于当时的技术限制以及知乎的动态渲染导致进度一直搁置。而这两天由于学校要做一个项目实践，我又重新思考了一下具体的实现方案，终于钻了知乎的一个空子，实现了这个百万级的爬虫。同时此次爬虫我是实实在在的进行了一定构思的，所以也对各个组件进行了分级、注释，所以在代码的可读性和结构性上也比之前要强很多。

​	首先大致说一下我们项目的需求吧。

1. **做一个百万级的用户数据爬虫，获取用户个人主页上的详细信息。**

2. **对着百万数据进行分析，并利用数据清洗工具和可视化工具给出一些的分析图表。**

3. **给使用者提供用户数据检索功能**

4. **给出单日用户人气排行【这里我们有一套自己的人气算法，该人气排行是每天更新热点问题中的活跃用户】**

5. **给用户提供一套可交互ui设计**

6. **使用者可以进入被检索用户的个人主页，下载该用户的全部数据，包括：回答，视频，专栏等**

​	而我作为本项目的数据开发员，主要担任的任务有：爬虫编写，数据分析，检索功能，以及排行算法的指定等。其中数据可视化由前端开发通过动态渲染达成。同时这里由于我们项目中定义的数据分析周期是1个月一次更新，所以这里我将原先的任务进行了分割，现阶段每日获取的用户数据量大概为5w-10w不等。这样一个月就有了150w的数据。当然后续还有对这个方案进行优化的打算，那个方案应该可以做到每日百万的效果吧。

​	然后就是最主要的这个爬虫的具体功能以及使用方法：

**功能**

- [x] 获取用户的详细信息【日均5w-10w】

- [ ] 获取用户问答，视频等详细数据
- [ ] 对上述数据提供下载
- [ ] 对获取的数据进行数据清洗以及分析
- [ ] 对单个被检索用户进行实时爬取

**使用方法**

- **git clone**

​	`https://github.com/srx-2000/git_spider.git`

* **安装依赖**

  需要使用到的库已经放在requirements.txt，使用pip安装的可以使用指令`pip install -r requirements.txt`。如果国内安装第三方库比较慢，可以使用以下指令进行清华源加速`pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/`

* **配置文件**

1. 打开`util/util_content.json`文件，配置以下是三个配置项，其中存储方式现版本仅提供使用CSV进行存储（后续版本会提供其他保存方式），cookies中保存个人的cookies，至少要有一个。cookie的具体获取方式见[连接](https://jingyan.baidu.com/article/5d368d1ea6c6e33f60c057ef.html)，thread_num则是线程数，一般开20左右就够用了，我这里自己测的时候，20个线程2分钟就能爬到1w的数据了。

   ```json
   {
     "save_method": "csv",
     "cookies": [
         
     ],
     "thread_num": 20
   }
   ```

2. 打开`zhihu_user_info\proxypool\config.yaml`配置ip池的host和端口 ，其中ip池的具体使用方式[详见](https://github.com/srx-2000/git_spider/tree/master/proxy_pool)

   ```yaml
   # https://github.com/jhao104/proxy_pool
   # 上述项目是本项目使用的代理池，原项目超级棒，而且在使用教学上也可以说是相当详细。
   # 一般来说将上述项目clone下来，在本机运行就可以了，如果想要自己配置一个服务器24小时的更新代理的话也可以将上述项目放到服务器上跑
   # 但要注意如果是放到服务器上跑，记得把下面的host改成自己服务器的公网IP
   
   # 主机地址，如果仅是在本机跑的话就改成127.0.0.1
   host: 127.0.0.1
   # 代理池接口端口，这里使用的是项目的默认端口
   port: 5010
   ```

* **运行爬虫（IP池需要启动详见上面链接）**

  * 使用pycharm运行：找到项目的spider.py文件，运行即可。

  * 使用命令行运行：进入项目根目录使用`python spider.py`即可运行

**运行效果**

1. **用户id获取**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/1.png)

2. **用户信息获取**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/2.png)

3. **最终结果**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/3.png)

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/4.png)

**爬取思路**

​	知乎作为大家熟知的问答社区，也被很多人当成了爬虫练手的基础站。但对于很多人都仅仅停留在对知乎的问答进行爬取，而这个可以说几乎没有难度，因为知乎本身也对这个没有设置什么反爬手段，也就是封个ip而已，随便搭个ip池就解决了。但知乎对于用户数据的防范却比问答多一些。之所以这么说是因为如果想要大批量的爬取用户信息的话无论如何都无法避免对用户的**传播方案**，而传统的传播方案无非就是对一个用户的粉丝和关注进行随机取数，然后不断递归下去。但知乎也料到了这个传播方式，所以使用了动态渲染对用户的关注和粉丝进行了屏蔽。同时如果想要直接通过知乎的接口获取数据就绕不开cookie【需要大量账号】，同时还有各种蜜汁请求头。

​	一开始我也被上述问题给难住了，这也就是为什么这个项目一直搁置的原因。可后来当我跳出这个思维定式之后就有了新的思路：爬取知乎的问答既然不需要cookie，同时也不会有奇怪的请求头，那么我为什么不可以把这个作为我的传播方案呢。既然理论成立，那么就开始实践，最终也证明了我的想法是正确的：每日定时获取当日的热榜信息，并爬取热榜中回答了问题的用户的uuid，再使用这个uuid获取具体的用户信息。这样虽然爬取量无法达到一次性的100w的数据，但却可以保证稳定高效。如果一个ip池中有10个ip的话，平均下来每个ip每天仅仅会请求250次左右，这个请求数量甚至可以让知乎都意识不到我是一个爬虫【知乎短时间的请求数达到500才会触发验证机制】，而这仅仅是10个ip的ip池，如果将池子扩大，同时增加一定的间隔，甚至可以做到24小时不间断无限爬取【但其实热榜的回答数量有限所以意义不大】。同时这个方案另一个很棒的点是对于cookie的消费会极小，因为cookie只会使用在对热榜的获取上，而这个请求其实可以几个小时进行一次，完全在一个真实用户的操作范围之内。

**更新日志**

* 2021.10.17

  更新第一版，完成每日用户详细信息获取